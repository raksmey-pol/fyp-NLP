{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ff82f3",
   "metadata": {},
   "source": [
    "# Fake News Detection - Model Training\n",
    "\n",
    "This notebook trains and compares multiple models:\n",
    "\n",
    "## Traditional ML Models:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n",
    "\n",
    "## Deep Learning Models:\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- BiLSTM (Bidirectional LSTM)\n",
    "- CNN-LSTM (Hybrid model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87efc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import issparse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43eabe1",
   "metadata": {},
   "source": [
    "## Part 1: Traditional ML Models with TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828fd093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF-IDF features...\n",
      "Training data: ()\n",
      "Validation data: ()\n",
      "Test data: ()\n"
     ]
    }
   ],
   "source": [
    "# Load TF-IDF features\n",
    "print(\"Loading TF-IDF features...\")\n",
    "\n",
    "X_train = np.load('../data/processed/features/tfidf/X_train.npy', allow_pickle=True)\n",
    "y_train = np.load('../data/processed/features/tfidf/y_train.npy', allow_pickle=True)\n",
    "X_val = np.load('../data/processed/features/tfidf/X_val.npy', allow_pickle=True)\n",
    "y_val = np.load('../data/processed/features/tfidf/y_val.npy', allow_pickle=True)\n",
    "X_test = np.load('../data/processed/features/tfidf/X_test.npy', allow_pickle=True)\n",
    "y_test = np.load('../data/processed/features/tfidf/y_test.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00f20d",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5fe0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ GPU Optimizations Enabled:\n",
      "   âœ… Mixed Precision (FP16) - 2-3x faster training\n",
      "   âœ… XLA compilation - Optimized GPU kernels\n",
      "   âœ… Memory growth - Efficient GPU memory usage\n",
      "âœ… Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "   Using GPU for training (GTX 1650 with 4GB VRAM)\n",
      "âœ… Found 1 GPU(s): ['/physical_device:GPU:0']\n",
      "   Using GPU for training (GTX 1650)\n",
      "Training logistic model...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Train Logistic Regression\u001b[39;00m\n\u001b[32m      4\u001b[39m lr_trainer = TraditionalModelTrainer(model_type=\u001b[33m'\u001b[39m\u001b[33mlogistic\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mlr_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m      8\u001b[39m lr_train_metrics = lr_trainer.evaluate(X_train, y_train, \u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Serenity/AUPP/ITM-454_NLP/Final_Project/fyp-NLP/notebooks/../src/models/traditional_models.py:87\u001b[39m, in \u001b[36mTraditionalModelTrainer.train\u001b[39m\u001b[34m(self, X_train, y_train, verbose)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type == \u001b[33m'\u001b[39m\u001b[33msvm\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from src.models.traditional_models import TraditionalModelTrainer\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_trainer = TraditionalModelTrainer(model_type='logistic')\n",
    "lr_trainer.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_train_metrics = lr_trainer.evaluate(X_train, y_train, 'Training')\n",
    "lr_val_metrics = lr_trainer.evaluate(X_val, y_val, 'Validation')\n",
    "lr_test_metrics = lr_trainer.evaluate(X_test, y_test, 'Test')\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lr_trainer.get_classification_report(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = lr_trainer.get_confusion_matrix(X_test, y_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])\n",
    "disp.plot(cmap='Blues', ax=ax)\n",
    "plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40c842",
   "metadata": {},
   "source": [
    "### 1.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd76348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_trainer = TraditionalModelTrainer(model_type='naive_bayes')\n",
    "nb_trainer.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "nb_train_metrics = nb_trainer.evaluate(X_train, y_train, 'Training')\n",
    "nb_val_metrics = nb_trainer.evaluate(X_val, y_val, 'Validation')\n",
    "nb_test_metrics = nb_trainer.evaluate(X_test, y_test, 'Test')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(nb_trainer.get_classification_report(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79d2ef",
   "metadata": {},
   "source": [
    "### 1.3 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (may take longer)\n",
    "print(\"Note: SVM training may take several minutes...\")\n",
    "\n",
    "svm_trainer = TraditionalModelTrainer(model_type='svm')\n",
    "svm_trainer.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "svm_train_metrics = svm_trainer.evaluate(X_train, y_train, 'Training')\n",
    "svm_val_metrics = svm_trainer.evaluate(X_val, y_val, 'Validation')\n",
    "svm_test_metrics = svm_trainer.evaluate(X_test, y_test, 'Test')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_trainer.get_classification_report(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d06fe7",
   "metadata": {},
   "source": [
    "### 1.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_trainer = TraditionalModelTrainer(model_type='random_forest')\n",
    "rf_trainer.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_train_metrics = rf_trainer.evaluate(X_train, y_train, 'Training')\n",
    "rf_val_metrics = rf_trainer.evaluate(X_val, y_val, 'Validation')\n",
    "rf_test_metrics = rf_trainer.evaluate(X_test, y_test, 'Test')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_trainer.get_classification_report(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c60f2",
   "metadata": {},
   "source": [
    "### 1.5 Compare Traditional ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "ml_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest'],\n",
    "    'Train Accuracy': [\n",
    "        lr_train_metrics['accuracy'],\n",
    "        nb_train_metrics['accuracy'],\n",
    "        svm_train_metrics['accuracy'],\n",
    "        rf_train_metrics['accuracy']\n",
    "    ],\n",
    "    'Val Accuracy': [\n",
    "        lr_val_metrics['accuracy'],\n",
    "        nb_val_metrics['accuracy'],\n",
    "        svm_val_metrics['accuracy'],\n",
    "        rf_val_metrics['accuracy']\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        lr_test_metrics['accuracy'],\n",
    "        nb_test_metrics['accuracy'],\n",
    "        svm_test_metrics['accuracy'],\n",
    "        rf_test_metrics['accuracy']\n",
    "    ],\n",
    "    'Test F1': [\n",
    "        lr_test_metrics['f1'],\n",
    "        nb_test_metrics['f1'],\n",
    "        svm_test_metrics['f1'],\n",
    "        rf_test_metrics['f1']\n",
    "    ],\n",
    "    'Test Precision': [\n",
    "        lr_test_metrics['precision'],\n",
    "        nb_test_metrics['precision'],\n",
    "        svm_test_metrics['precision'],\n",
    "        rf_test_metrics['precision']\n",
    "    ],\n",
    "    'Test Recall': [\n",
    "        lr_test_metrics['recall'],\n",
    "        nb_test_metrics['recall'],\n",
    "        svm_test_metrics['recall'],\n",
    "        rf_test_metrics['recall']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRADITIONAL ML MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(ml_results.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Test Accuracy', 'Test F1', 'Test Precision', 'Test Recall']\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'plum']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.barh(ml_results['Model'], ml_results[metric], color=color, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(ml_results[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fde33b",
   "metadata": {},
   "source": [
    "## Part 2: Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed text data\n",
    "print(\"Loading preprocessed data for deep learning...\")\n",
    "\n",
    "df = pd.read_csv('../data/processed/processed_news.csv')\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
    "train, val = train_test_split(train_val, test_size=0.15/0.85, random_state=42, stratify=train_val['label'])\n",
    "\n",
    "texts_train = train['cleaned_text'].values\n",
    "y_train_dl = train['label'].values\n",
    "texts_val = val['cleaned_text'].values\n",
    "y_val_dl = val['label'].values\n",
    "texts_test = test['cleaned_text'].values\n",
    "y_test_dl = test['label'].values\n",
    "\n",
    "print(f\"Training samples: {len(texts_train)}\")\n",
    "print(f\"Validation samples: {len(texts_val)}\")\n",
    "print(f\"Test samples: {len(texts_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95cb57",
   "metadata": {},
   "source": [
    "### 2.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.deep_learning_models import LSTMModel\n",
    "\n",
    "# Initialize LSTM\n",
    "lstm_model = LSTMModel(max_features=10000, embedding_dim=128, max_length=500)\n",
    "\n",
    "# Prepare data\n",
    "print(\"Preparing data...\")\n",
    "X_train_lstm, y_train_lstm = lstm_model.prepare_data(texts_train, y_train_dl)\n",
    "X_val_lstm = lstm_model.prepare_data(texts_val)\n",
    "X_test_lstm = lstm_model.prepare_data(texts_test)\n",
    "\n",
    "# Build and train\n",
    "lstm_model.build_model()\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "lstm_model.model.summary()\n",
    "\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm_history = lstm_model.train(X_train_lstm, y_train_lstm, X_val_lstm, y_val_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "lstm_test_metrics = lstm_model.evaluate(X_test_lstm, y_test_dl)\n",
    "\n",
    "print(\"\\nLSTM Test Metrics:\")\n",
    "print(f\"  Accuracy:  {lstm_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {lstm_test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {lstm_test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {lstm_test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "lstm_model.save('lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec53714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(lstm_history.history['accuracy'], label='Train', marker='o')\n",
    "axes[0].plot(lstm_history.history['val_accuracy'], label='Validation', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('LSTM - Training & Validation Accuracy', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(lstm_history.history['loss'], label='Train', marker='o')\n",
    "axes[1].plot(lstm_history.history['val_loss'], label='Validation', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('LSTM - Training & Validation Loss', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210f93b",
   "metadata": {},
   "source": [
    "### 2.2 BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd585c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.deep_learning_models import BiLSTMModel\n",
    "\n",
    "# Initialize BiLSTM\n",
    "bilstm_model = BiLSTMModel(max_features=10000, embedding_dim=128, max_length=500)\n",
    "\n",
    "# Prepare data\n",
    "X_train_bilstm, y_train_bilstm = bilstm_model.prepare_data(texts_train, y_train_dl)\n",
    "X_val_bilstm = bilstm_model.prepare_data(texts_val)\n",
    "X_test_bilstm = bilstm_model.prepare_data(texts_test)\n",
    "\n",
    "# Build and train\n",
    "bilstm_model.build_model()\n",
    "print(\"\\nBiLSTM Model Architecture:\")\n",
    "bilstm_model.model.summary()\n",
    "\n",
    "print(\"\\nTraining BiLSTM...\")\n",
    "bilstm_history = bilstm_model.train(X_train_bilstm, y_train_bilstm, X_val_bilstm, y_val_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc055b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BiLSTM\n",
    "bilstm_test_metrics = bilstm_model.evaluate(X_test_bilstm, y_test_dl)\n",
    "\n",
    "print(\"\\nBiLSTM Test Metrics:\")\n",
    "print(f\"  Accuracy:  {bilstm_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {bilstm_test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {bilstm_test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {bilstm_test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "bilstm_model.save('bilstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b61bf",
   "metadata": {},
   "source": [
    "### 2.3 CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.deep_learning_models import CNNLSTMModel\n",
    "\n",
    "# Initialize CNN-LSTM\n",
    "cnn_lstm_model = CNNLSTMModel(max_features=10000, embedding_dim=128, max_length=500)\n",
    "\n",
    "# Prepare data\n",
    "X_train_cnn, y_train_cnn = cnn_lstm_model.prepare_data(texts_train, y_train_dl)\n",
    "X_val_cnn = cnn_lstm_model.prepare_data(texts_val)\n",
    "X_test_cnn = cnn_lstm_model.prepare_data(texts_test)\n",
    "\n",
    "# Build and train\n",
    "cnn_lstm_model.build_model()\n",
    "print(\"\\nCNN-LSTM Model Architecture:\")\n",
    "cnn_lstm_model.model.summary()\n",
    "\n",
    "print(\"\\nTraining CNN-LSTM...\")\n",
    "cnn_lstm_history = cnn_lstm_model.train(X_train_cnn, y_train_cnn, X_val_cnn, y_val_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a754a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN-LSTM\n",
    "cnn_lstm_test_metrics = cnn_lstm_model.evaluate(X_test_cnn, y_test_dl)\n",
    "\n",
    "print(\"\\nCNN-LSTM Test Metrics:\")\n",
    "print(f\"  Accuracy:  {cnn_lstm_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {cnn_lstm_test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {cnn_lstm_test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {cnn_lstm_test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "cnn_lstm_model.save('cnn_lstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b203e1",
   "metadata": {},
   "source": [
    "## Part 3: Overall Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest',\n",
    "        'LSTM', 'BiLSTM', 'CNN-LSTM'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'ML', 'ML', 'ML', 'ML',\n",
    "        'DL', 'DL', 'DL'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        lr_test_metrics['accuracy'],\n",
    "        nb_test_metrics['accuracy'],\n",
    "        svm_test_metrics['accuracy'],\n",
    "        rf_test_metrics['accuracy'],\n",
    "        lstm_test_metrics['accuracy'],\n",
    "        bilstm_test_metrics['accuracy'],\n",
    "        cnn_lstm_test_metrics['accuracy']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        lr_test_metrics['f1'],\n",
    "        nb_test_metrics['f1'],\n",
    "        svm_test_metrics['f1'],\n",
    "        rf_test_metrics['f1'],\n",
    "        lstm_test_metrics['f1'],\n",
    "        bilstm_test_metrics['f1'],\n",
    "        cnn_lstm_test_metrics['f1']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        lr_test_metrics['precision'],\n",
    "        nb_test_metrics['precision'],\n",
    "        svm_test_metrics['precision'],\n",
    "        rf_test_metrics['precision'],\n",
    "        lstm_test_metrics['precision'],\n",
    "        bilstm_test_metrics['precision'],\n",
    "        cnn_lstm_test_metrics['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        lr_test_metrics['recall'],\n",
    "        nb_test_metrics['recall'],\n",
    "        svm_test_metrics['recall'],\n",
    "        rf_test_metrics['recall'],\n",
    "        lstm_test_metrics['recall'],\n",
    "        bilstm_test_metrics['recall'],\n",
    "        cnn_lstm_test_metrics['recall']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "all_results = all_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPLETE MODEL COMPARISON (Test Set Performance)\")\n",
    "print(\"=\"*100)\n",
    "print(all_results.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results\n",
    "all_results.to_csv('../results/model_comparison.csv', index=False)\n",
    "print(\"\\nâœ… Results saved to: results/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all models\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(all_results))\n",
    "width = 0.2\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'plum']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    offset = width * (i - 1.5)\n",
    "    ax.bar(x + offset, all_results[metric], width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Complete Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_results['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Comparison plot saved to: results/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99263d74",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Performing Model**: Check the sorted table above\n",
    "2. **Traditional ML**: Fast training, good baseline performance\n",
    "3. **Deep Learning**: Better at capturing sequential patterns\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Phase 5: Model Evaluation & Analysis\n",
    "2. Phase 6: Model Optimization & Hyperparameter Tuning\n",
    "3. Phase 7: Deployment & Documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
